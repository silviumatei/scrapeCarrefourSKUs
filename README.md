# scrapeCarrefourSKUs
For market researchers in need of all the skus in a category to list in a questionnaire without getting banned by hypermarkets
-----

## Carrefour KSA Product Scraper & Data Processor

This repository contains a set of Python scripts designed to scrape product names and image links from the Carrefour KSA website, merge the collected data, and export it into a clean CSV format. A key feature of this scraper is its ability to connect to an **already open Chrome browser page**, which can help in avoiding common anti-bot measures like IP bans, especially for prolonged scraping sessions.

### Files in this Repository

1.  **`carrefour_scraper.ipynb` (or `.py`)**:

      * **Purpose**: This script is the core scraper. It connects to your pre-configured and open Chrome browser instance, navigates to the specified product category page (e.g., Chips, Puffs, Popcorn), scrolls down to load all available products, and then extracts product names and their direct image URLs.
      * **Key Feature**: You may add a function to also download the product images locally. The existing code only captures the publicly accessible image URLs directly.
      * **Output**: Saves the scraped data for a specific category into a JSON file (e.g., `snacksKSA.json`, `puffsKSA.json`).

2.  **`merge_jsons.ipynb` (or `.py`)**:

      * **Purpose**: This utility script takes multiple JSON files generated by the `carrefour_scraper.ipynb` (or `.py`) and merges them into a single, comprehensive JSON file.
      * **Usage**: Useful when you've scraped different categories into separate files and want a unified dataset.
      * **Output**: A single JSON file (e.g., `allSnacksKSA.json`) containing all collected product data.

3.  **`export_to_csv.ipynb` (or `.py`)**:

      * **Purpose**: This script converts the merged JSON file (or any single JSON file containing a list of objects) into a more universally usable CSV format.
      * **Output**: A CSV file (e.g., `allSnacksKSA.csv`) ready for analysis or further processing in spreadsheet software.

### Setup and Requirements

It is highly recommended to set up a dedicated Python environment for this project, ideally using `Miniconda` or `Anaconda`, to manage dependencies effectively.

#### 1\. Software Prerequisites:

  * **Google Chrome Browser**: Ensure you have Google Chrome installed on your system.
  * **Miniconda (Recommended)**: Download and install Miniconda for your operating system. This provides `conda` for environment management and `pip` for package installation.
  * **Python 3.11**: This project is developed with Python 3.11. While other Python 3.x versions might work, 3.11 is recommended.
  * **Jupyter Notebook / JupyterLab**: Essential for running `.ipynb` files interactively.

#### 2\. Environment Setup (using Miniconda):

1.  **Create a new conda environment**:
    ```bash
    conda create -n carrefour_scrape python=3.11
    ```
2.  **Activate the environment**:
    ```bash
    conda activate carrefour_scrape
    ```

#### 3\. Install Python Packages:

Navigate to your project's root directory in the activated conda environment and install the required packages using the `requirements.txt` file provided.

```bash
pip install -r requirements.txt
```

#### `requirements.txt` content:

```
selenium>=4.0.0
pandas>=1.0.0
jupyter>=1.0.0
```

#### 4\. Configure Chrome for Remote Debugging:

This is the most crucial step for the scraper to connect to your open browser.

1.  **Choose a Chrome Profile Path**: Decide where you want your dedicated Chrome profile to be stored. For example: `L:\temp\scrapellm\chrome_profile`.

2.  **Launch Chrome from Command Line**: Close all existing Chrome instances. Then, open your command prompt (Windows) or terminal (macOS/Linux) and launch Chrome using the following command. **Replace the `user-data-dir` path with your chosen path.**

      * **Windows**:

        ```bash
        "C:\Program Files\Google\Chrome\Application\chrome.exe" --remote-debugging-port=9222 --user-data-dir="L:\temp\scrapellm\chrome_profile"
        ```

        (Adjust `"C:\Program Files\Google\Chrome\Application\chrome.exe"` if your Chrome executable is in a different location.)

      * **macOS**:

        ```bash
        /Applications/Google\ Chrome.app/Contents/MacOS/Google\ Chrome --remote-debugging-port=9222 --user-data-dir="L:\temp\scrapellm\chrome_profile"
        ```

      * **Linux**:

        ```bash
        google-chrome --remote-debugging-port=9222 --user-data-dir="/path/to/your/chrome_profile"
        ```

    This will open a new Chrome window (or use an existing one if correctly configured and only one instance is running). **Do not close this window.** You can navigate to the Carrefour KSA page manually within this window if you wish, or let the script do it. I recommend you do it!

#### 5\. Running the Scripts:

1.  **Start Jupyter (optional, but recommended for `.ipynb` files)**:

    ```bash
    jupyter notebook
    ```
    This will open a browser tab with the Jupyter interface.

2.  **Execute the `carrefour_scraper.ipynb` (or `.py`)**:

      * Open the notebook.
      * Ensure the `GOOGLE_CHROME_PROFILE_PATH` and `GOOGLE_CHROME_DEBUGGING_PORT` variables in the script match your setup.
      * Run the cells sequentially. The script will connect to your open Chrome instance and start scraping. You can see the log on your screen.

3.  **Execute `merge_jsons.ipynb` (or `.py`)**:

      * After scraping multiple categories, run this script to combine your JSON files.
      * Adjust the `json_files_to_merge` list in the script to include all your scraped JSON files.

4.  **Execute `export_to_csv.ipynb` (or `.py`)**:

      * Finally, run this script to convert your merged JSON into a CSV.
      * Ensure `INPUT_JSON_FILE` points to your merged JSON file.

-----
