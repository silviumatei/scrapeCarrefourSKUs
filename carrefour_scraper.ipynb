{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d75e7d-4209-419f-87e7-146c2ec710f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saves the names and image links only - chips\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options as ChromeOptions\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, StaleElementReferenceException\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import atexit\n",
    "# Removed requests import as it's no longer directly used for image download\n",
    "# Removed urlsplit import as it was used with requests\n",
    "\n",
    "# --- Configuration ---\n",
    "# --- GOOGLE CHROME SPECIFIC PATHS ---\n",
    "GOOGLE_CHROME_PROFILE_PATH = r\"L:\\temp\\scrapellm\\chrome_profile\" # The user data directory for Google Chrome profile\n",
    "GOOGLE_CHROME_DEBUGGING_PORT = 9222 # Standard port for Chrome/Chromium remote debugging\n",
    "\n",
    "# Output directory for logs, and JSON data\n",
    "BASE_OUTPUT_DIR = r\"D:\\Docs\\data\"\n",
    "OUTPUT_JSON_FILE = os.path.join(BASE_OUTPUT_DIR, \"snacksKSA.json\")\n",
    "LOG_FILE = os.path.join(BASE_OUTPUT_DIR, \"automation_log.log\")\n",
    "# We no longer need SCREENSHOT_DIR or IMAGE_SAVE_DIR for saving files,\n",
    "# but keeping their creation for minimal disruption to existing paths.\n",
    "# No images will actually be saved to them with this version.\n",
    "SCREENSHOT_DIR = os.path.join(BASE_OUTPUT_DIR, \"screenshots\") \n",
    "IMAGE_SAVE_DIR = os.path.join(BASE_OUTPUT_DIR, \"product_images\")\n",
    "\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "os.makedirs(BASE_OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(SCREENSHOT_DIR, exist_ok=True)\n",
    "os.makedirs(IMAGE_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# --- Logging Setup ---\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(LOG_FILE),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "global_driver = None\n",
    "\n",
    "def cleanup_driver():\n",
    "    global global_driver\n",
    "    if global_driver:\n",
    "        try:\n",
    "            logger.info(\"Attempting to close WebDriver gracefully via atexit.\")\n",
    "            global_driver.quit()\n",
    "            logger.info(\"WebDriver closed successfully by atexit handler.\")\n",
    "            global_driver = None\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error closing WebDriver via atexit handler: {e}\")\n",
    "\n",
    "atexit.register(cleanup_driver)\n",
    "\n",
    "def initialize_driver():\n",
    "    \"\"\"Initializes Selenium WebDriver to connect to an existing Google Chrome instance.\"\"\"\n",
    "    global global_driver\n",
    "    try:\n",
    "        chrome_options = ChromeOptions()\n",
    "        \n",
    "        # Connect to an existing Google Chrome instance via remote debugging port\n",
    "        chrome_options.add_experimental_option(\"debuggerAddress\", f\"localhost:{GOOGLE_CHROME_DEBUGGING_PORT}\")\n",
    "        \n",
    "        # Specify the user data directory (profile path)\n",
    "        chrome_options.add_argument(f\"--user-data-dir={GOOGLE_CHROME_PROFILE_PATH}\")\n",
    "\n",
    "        # Initialize the Chrome WebDriver\n",
    "        global_driver = webdriver.Chrome(options=chrome_options)\n",
    "        \n",
    "        global_driver.maximize_window()\n",
    "        time.sleep(random.uniform(1.0, 3.0)) # Give browser a moment to initialize and load\n",
    "        logger.info(\"Selenium ChromeDriver (Google Chrome) initialized successfully and connected to existing instance.\")\n",
    "        return global_driver\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Failed to initialize Google Chrome WebDriver. Ensure Google Chrome is running with remote debugging enabled (--remote-debugging-port={GOOGLE_CHROME_DEBUGGING_PORT} --user-data-dir=\\\"{GOOGLE_CHROME_PROFILE_PATH}\\\"), and no other Chrome instances are using this port or profile. Error: {e}\", exc_info=True)\n",
    "        raise # Re-raise to stop the script if driver can't start\n",
    "\n",
    "# --- SIMPLIFIED save_image_locally function to ONLY return the image URL ---\n",
    "def save_image_locally(driver, image_url, product_name):\n",
    "    \"\"\"\n",
    "    This function no longer attempts to download or save images locally.\n",
    "    It simply returns the provided image_url string for inclusion in the JSON.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Using image URL directly for '{product_name}': {image_url}\")\n",
    "    return image_url # Return the URL directly for the JSON output\n",
    "\n",
    "# --- scrape_carrefour_products function (Copied EXACTLY from your \"working code\" snippet) ---\n",
    "def scrape_carrefour_products(driver):\n",
    "    \"\"\"\n",
    "    Scrapes product names and image URLs from the Carrefour page.\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting product scraping with provided working XPaths and collecting image URLs.\")\n",
    "    \n",
    "    products_data = []\n",
    "    \n",
    "    # --- XPaths from the code you indicated was previously working for initial scrape ---\n",
    "    main_product_list_container_xpath = \"/html/body/main/div/div[2]/div[2]/div/div[2]\"\n",
    "    \n",
    "    product_row_xpath = f\"{main_product_list_container_xpath}//div[contains(@class, 'mb-lg') and contains(@class, 'flex') and contains(@class, 'w-fit') and contains(@class, 'justify-start') and contains(@class, 'gap-4')]\"\n",
    "    \n",
    "    product_card_in_row_xpath = \".//div[contains(@class, 'relative') and contains(@class, 'w-[134px]') and contains(@class, 'flex') and contains(@class, 'justify-between')]\"\n",
    "    \n",
    "    try:\n",
    "        # 1. Wait for the main product list container to be present.\n",
    "        WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_element_located((By.XPATH, main_product_list_container_xpath))\n",
    "        )\n",
    "        logger.info(f\"Main product list container located using: {main_product_list_container_xpath}\")\n",
    "\n",
    "        # 2. Find all product rows\n",
    "        logger.info(f\"Attempting to find all product rows using XPath: {product_row_xpath}\")\n",
    "        product_rows = WebDriverWait(driver, 30).until( \n",
    "            EC.presence_of_all_elements_located((By.XPATH, product_row_xpath))\n",
    "        )\n",
    "        logger.info(f\"Found {len(product_rows)} product rows on the page.\")\n",
    "\n",
    "        if not product_rows:\n",
    "            logger.warning(\"No product rows found. Check the product_row_xpath or if content is dynamically loaded differently.\")\n",
    "            return products_data \n",
    "\n",
    "        # 3. Iterate through each row and find product cards within them\n",
    "        for row_index, row_element in enumerate(product_rows):\n",
    "            logger.info(f\"Processing row {row_index + 1}/{len(product_rows)}\")\n",
    "            \n",
    "            # Find all individual product cards within the current row\n",
    "            try:\n",
    "                cards_in_row = WebDriverWait(row_element, 5).until( \n",
    "                    EC.presence_of_all_elements_located((By.XPATH, product_card_in_row_xpath))\n",
    "                )\n",
    "                logger.debug(f\"Found {len(cards_in_row)} product cards in row {row_index + 1}.\")\n",
    "            except TimeoutException:\n",
    "                logger.warning(f\"No product cards found within row {row_index + 1} after 5 seconds. Skipping this row. Row outerHTML (first 500 chars): {row_element.get_attribute('outerHTML')[:500]}...\")\n",
    "                continue \n",
    "\n",
    "            for card_index, card_element in enumerate(cards_in_row):\n",
    "                product_name = None\n",
    "                image_url = None\n",
    "                \n",
    "                try:\n",
    "                    # Use a short WebDriverWait for elements *within* the card, as the card element is already present\n",
    "                    wait_in_card = WebDriverWait(card_element, 2) \n",
    "                    \n",
    "                    logger.debug(f\"Scraping product card {card_index + 1} in row {row_index + 1}.\")\n",
    "\n",
    "                    # Product Name:\n",
    "                    name_element = wait_in_card.until(\n",
    "                        EC.presence_of_element_located((By.XPATH, \".//div[contains(@class, 'line-clamp-2')]/span\"))\n",
    "                    )\n",
    "                    product_name = name_element.text.strip()\n",
    "                    \n",
    "                    # Product Image:\n",
    "                    image_element = wait_in_card.until(\n",
    "                        EC.presence_of_element_located((By.XPATH, \".//div[contains(@class, 'relative')]/a/div/img\"))\n",
    "                    )\n",
    "                    image_url = image_element.get_attribute(\"src\")\n",
    "\n",
    "                    if product_name and image_url:\n",
    "                        logger.info(f\"Scraped - Name: '{product_name}', Image URL: '{image_url}' (Row {row_index + 1}, Card {card_index + 1})\")\n",
    "                        # Call save_image_locally which now just returns the URL string\n",
    "                        final_image_link = save_image_locally(driver, image_url, product_name) \n",
    "                        \n",
    "                        products_data.append({\n",
    "                            \"source\": \"carrefour\",\n",
    "                            \"link\": driver.current_url, # This will be the category page URL\n",
    "                            \"category\": \"chips\", \n",
    "                            \"name\": product_name,\n",
    "                            \"image\": final_image_link # This will now be the image URL string\n",
    "                        })\n",
    "                    else:\n",
    "                        logger.warning(f\"Missing name or image URL for product in Row {row_index + 1}, Card {card_index + 1}. Name: '{product_name}', Image URL: '{image_url}'\")\n",
    "\n",
    "                except (TimeoutException, NoSuchElementException, StaleElementReferenceException) as e:\n",
    "                    logger.warning(f\"Element not found or stale for product in Row {row_index + 1}, Card {card_index + 1}. Error: {e}\")\n",
    "                    try: \n",
    "                        logger.debug(f\"Card {card_index+1} outer HTML (first 500 chars): {card_element.get_attribute('outerHTML')[:500]}...\")\n",
    "                    except Exception as get_html_e:\n",
    "                        logger.warning(f\"Error getting outerHTML for card {card_index+1}: {get_html_e}\")\n",
    "                    continue \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"An unexpected error occurred while processing product in Row {row_index + 1}, Card {card_index + 1}: {e}\", exc_info=True)\n",
    "                    continue \n",
    "\n",
    "    except (TimeoutException, NoSuchElementException) as e:\n",
    "        logger.critical(f\"CRITICAL: Failed to find main product content area or any product rows. Error: {e}\", exc_info=True)\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"An unhandled critical error occurred during the overall product scraping process: {e}\", exc_info=True)\n",
    "    \n",
    "    return products_data\n",
    "\n",
    "def main():\n",
    "    driver = None\n",
    "    target_url = \"https://www.carrefourksa.com/mafsau/en/c/FKSA1730000?currentPage=0&filter=product_category_level_3_en%3A%27FKSA1730200%27&nextPageOffset=0&pageSize=200&sortBy=relevance\"\n",
    "\n",
    "    try:\n",
    "        driver = initialize_driver()\n",
    "        \n",
    "        if driver.current_url != target_url:\n",
    "            logger.info(f\"Current URL ({driver.current_url}) does not match target. Navigating to: {target_url}\")\n",
    "            driver.get(target_url)\n",
    "            logger.info(f\"Navigated to: {target_url}\")\n",
    "            time.sleep(random.uniform(3, 5)) \n",
    "        else:\n",
    "            logger.info(f\"Already on the target URL: {target_url}. Skipping navigation.\")\n",
    "            time.sleep(random.uniform(1, 2))\n",
    "\n",
    "        logger.info(\"Starting scroll to ensure all products are loaded...\")\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        scroll_attempts = 0\n",
    "        max_scroll_attempts = 15 \n",
    "        while scroll_attempts < max_scroll_attempts:\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(random.uniform(2, 4)) \n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                logger.info(\"Reached bottom of the page (no new content loaded after scroll).\")\n",
    "                break\n",
    "            last_height = new_height\n",
    "            scroll_attempts += 1\n",
    "            logger.info(f\"Scrolled. Current height: {new_height}, Scroll attempt: {scroll_attempts}\")\n",
    "        if scroll_attempts == max_scroll_attempts:\n",
    "            logger.warning(f\"Max scroll attempts ({max_scroll_attempts}) reached. Page might not have fully loaded or is infinite.\")\n",
    "        \n",
    "        time.sleep(random.uniform(1, 2)) \n",
    "\n",
    "        scraped_products = scrape_carrefour_products(driver)\n",
    "\n",
    "        try:\n",
    "            with open(OUTPUT_JSON_FILE, 'w', encoding='utf-8') as f:\n",
    "                json.dump(scraped_products, f, ensure_ascii=False, indent=4)\n",
    "            logger.info(f\"Scraped data saved to {OUTPUT_JSON_FILE}\")\n",
    "            logger.info(f\"Total products scraped: {len(scraped_products)}\")\n",
    "        except IOError as e:\n",
    "            logger.error(f\"Error saving data to JSON file: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Script encountered a critical error in main execution: {e}\", exc_info=True)\n",
    "    finally:\n",
    "        pass \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:scrape]",
   "language": "python",
   "name": "conda-env-scrape-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
